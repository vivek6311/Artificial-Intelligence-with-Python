{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1d2Qhg129BTIxkLTch_ycC078DagP5xad",
      "authorship_tag": "ABX9TyM+u78SrgNjavXIF6pwXhVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivek6311/Artificial-Intelligence-with-Python/blob/master/Least_Square_Regression_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the crop yield data (replace with your own data)\n",
        "X = np.random.rand(100, 5)  # Input features\n",
        "y = np.random.rand(100)     # Target variable (crop yield)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the PLS regression model\n",
        "n_components = 2  # Number of components for PLS\n",
        "pls = PLSRegression(n_components=n_components)\n",
        "pls.fit(X_train, y_train)\n",
        "\n",
        "# Predict crop yield on the testing set\n",
        "y_pred = pls.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"R-squared (R2) Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoDT770KIQ7_",
        "outputId": "ac9c48ff-88fd-4059-f7ae-bffb4f76990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.08260240540315757\n",
            "R-squared (R2) Score: -0.0559913901384721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the NDVI data into a pandas dataframe\n",
        "ndvi_data = pd.read_excel('/content/NDVI_data_blocks.xlsx')  # Replace 'ndvi_data.csv' with the actual filename or path of your NDVI data\n",
        "\n",
        "# Convert the date column to datetime format\n",
        "ndvi_data['Date'] = pd.to_datetime(ndvi_data['Date'])\n",
        "\n",
        "# Calculate the mean NDVI for each date\n",
        "ndvi_data['Mean_NDVI'] = ndvi_data.iloc[:, 1:].mean(axis=1)\n",
        "\n",
        "# Find the peak NDVI value and corresponding date\n",
        "peak_ndvi_row = ndvi_data.loc[ndvi_data['Mean_NDVI'].idxmax()]\n",
        "peak_ndvi_date = peak_ndvi_row['Date']\n",
        "\n",
        "# Determine the harvesting date\n",
        "harvesting_date = peak_ndvi_date + pd.DateOffset(days=45)  # Assuming 45 days after the peak NDVI as the harvesting date\n",
        "\n",
        "# Print the harvesting date\n",
        "print('Harvesting Date:', harvesting_date)\n"
      ],
      "metadata": {
        "id": "-j0i3yzW-ISi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6276cc0-e816-4384-e5b3-d031c7d61320"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harvesting Date: 2023-03-29 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with the available 40 data points\n",
        "# The DataFrame should have the following columns: 'Date', 'Column1', 'Column2', ...\n",
        "\n",
        "# Create a new DataFrame with all dates from the start to end of the year\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "all_dates = pd.date_range(start=start_date, end=end_date + pd.DateOffset(days=365))\n",
        "\n",
        "# Perform linear interpolation for each column\n",
        "interpolated_data = pd.DataFrame({'Date': all_dates})\n",
        "\n",
        "for column in df.columns[1:]:\n",
        "    column_data = df[['Date', column]].copy()\n",
        "    column_data = column_data.sort_values('Date').reset_index(drop=True)\n",
        "    column_data['Date'] = column_data['Date'].astype('int64') // 10**9  # Convert dates to integer timestamps\n",
        "\n",
        "    # Perform linear interpolation\n",
        "    interpolated_values = np.interp(all_dates.astype('int64') // 10**9, column_data['Date'], column_data[column])\n",
        "    interpolated_data[column] = interpolated_values\n",
        "\n",
        "# Save the interpolated data to an Excel file\n",
        "interpolated_data.to_excel('interpolated_data.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "iE6DS3Ji-ZoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF-_qA6nHzBk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with the available 40 data points\n",
        "# The DataFrame should have the following columns: 'Date', 'Column1', 'Column2', ...\n",
        "\n",
        "# Convert the date column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n",
        "\n",
        "# Create a new DataFrame with all dates from the start to end of the year\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "all_dates = pd.date_range(start=start_date, end=end_date + pd.DateOffset(days=365))\n",
        "\n",
        "# Perform linear interpolation for each column\n",
        "interpolated_data = pd.DataFrame({'Date': all_dates})\n",
        "\n",
        "for column in df.columns[1:]:\n",
        "    column_data = df[['Date', column]].copy()\n",
        "    column_data = column_data.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # Perform linear interpolation\n",
        "    interpolated_values = np.interp(all_dates, column_data['Date'], column_data[column])\n",
        "    interpolated_data[column] = interpolated_values\n",
        "\n",
        "# Save the interpolated data to an Excel file\n",
        "interpolated_data.to_excel('interpolated_data.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# List of Excel files containing the data sheets\n",
        "excel_files = ['file1.xlsx', 'file2.xlsx', 'file3.xlsx']\n",
        "\n",
        "# Iterate over each Excel file\n",
        "for file in excel_files:\n",
        "    # Read the Excel file and get the sheet names\n",
        "    sheets = pd.read_excel(file, sheet_name=None)\n",
        "    \n",
        "    # Iterate over each sheet in the Excel file\n",
        "    for sheet_name, df in sheets.items():\n",
        "        # Convert the date column to datetime format\n",
        "        df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n",
        "        \n",
        "        # Create a new DataFrame with all dates from the start to end of the year\n",
        "        start_date = df['Date'].min()\n",
        "        end_date = df['Date'].max()\n",
        "        all_dates = pd.date_range(start=start_date, end=end_date + pd.DateOffset(days=365))\n",
        "        \n",
        "        # Perform linear interpolation for each column\n",
        "        interpolated_data = pd.DataFrame({'Date': all_dates})\n",
        "        \n",
        "        for column in df.columns[1:]:\n",
        "            column_data = df[['Date', column]].copy()\n",
        "            column_data = column_data.sort_values('Date').reset_index(drop=True)\n",
        "            \n",
        "            # Perform linear interpolation\n",
        "            interpolated_values = np.interp(all_dates, column_data['Date'], column_data[column])\n",
        "            interpolated_data[column] = interpolated_values\n",
        "        \n",
        "        # Save the interpolated data to an Excel file\n",
        "        output_file = f'interpolated_{file}_{sheet_name}.xlsx'\n",
        "        interpolated_data.to_excel(output_file, index=False)\n",
        "        print(f'Interpolated data saved to {output_file}')\n"
      ],
      "metadata": {
        "id": "LyBDa--OJMVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Interpolation for the given set of data"
      ],
      "metadata": {
        "id": "iVdYf_eoJKcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Excel file name\n",
        "excel_file = \"All_blocks.xlsx\"\n",
        "\n",
        "# Read the Excel file and get the sheet names\n",
        "sheets = pd.read_excel(excel_file, sheet_name=None)\n",
        "\n",
        "# Iterate over each sheet in the Excel file\n",
        "for sheet_name, df in sheets.items():\n",
        "    # Convert the date column to datetime format\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n",
        "    \n",
        "    # Create a new DataFrame with all dates from the start to end of the year\n",
        "    start_date = df['Date'].min()\n",
        "    end_date = df['Date'].max()\n",
        "    all_dates = pd.date_range(start=start_date, end=end_date + pd.DateOffset(days=365))\n",
        "    \n",
        "    # Perform linear interpolation for each column\n",
        "    interpolated_data = pd.DataFrame({'Date': all_dates})\n",
        "    \n",
        "    for column in df.columns[1:]:\n",
        "        column_data = df[['Date', column]].copy()\n",
        "        column_data = column_data.sort_values('Date').reset_index(drop=True)\n",
        "        \n",
        "        # Perform linear interpolation\n",
        "        interpolated_values = np.interp(all_dates, column_data['Date'], column_data[column])\n",
        "        interpolated_data[column] = interpolated_values\n",
        "    \n",
        "    # Save the interpolated data to an Excel file\n",
        "    output_file = f'interpolated_{excel_file}_{sheet_name}.xlsx'\n",
        "    interpolated_data.to_excel(output_file, index=False)\n",
        "    print(f'Interpolated data saved to {output_file}')\n"
      ],
      "metadata": {
        "id": "T3g3Wmj0JB__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d5a729-d712-4994-a2fe-7ccb9a1f0441"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolated data saved to interpolated_All_blocks.xlsx_AVG_All_Block_CVI.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_All_Block_EVI_data.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_All_Block_EVI2.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_GLI_data.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_GNDVI_data.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_NDVI_data.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_NDWI_data.xlsx\n",
            "Interpolated data saved to interpolated_All_blocks.xlsx_SAVI.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nOnJARaKrXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "f4feEoTjK1aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "5mET_vCYJCc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atug2OGmLGBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XwfCo_VLGmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSDa6dvs9kc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "g9L9lok3PYVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IINyWuJ-NYnt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}